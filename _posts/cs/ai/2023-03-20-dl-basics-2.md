---
layout: post
title: ìµœì í™”
category: cs
tags: ai

# ìµœì í™”

# ìµœì í™” ê¸°ë²•

- 30ë¶„ ë‚´ ë‹´ê¸°ì— ë„ˆë¬´ ë°©ëŒ€í•œ ì£¼ì œ
- ê°ê°ì˜ ë¶„ì•¼ë§ˆë‹¤ ìµœì†Œ í•œ í•™ê¸° ì´ìƒ ê³µë¶€í•˜ëŠ” ë¶„ì•¼
- ì¤„ì´ê³  ì¤„ì—¬ì„œ ì‹¤ì œë¡œ ì ìš© ì‹œ ì¤‘ìš”í•˜ê²Œ ì•Œì•„ë‘ì–´ì•¼ í•  ê°œë…ë“¤ ìœ„ì£¼ë¡œ

## Introduction

<aside>
ğŸ’¡ â€œLanguage is the source of misunderstanding,â€ Antoine de Saint-ExupÃ¨ry (1900~1944)

</aside>

### Gradient Descent

- 1st-order iterative optimization algorithm for finding a local minimum of a differentiable function.

### Important Concepts in Optimization

- Generalization
- Underfitting vs. overfitting
- Cross-validation
- Bias-variance trade-off
- Bootstrapping
- Bagging and boosting

## Generalization

- How well the learned model will behave on unseen data.

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled.png)

## Underfitting vs. Overfitting

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%201.png)

## Cross-validation

- Cross-validation is a model validation technique for assessing how the model will generalize to an independent (test) dataset.

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%202.png)

- CVë¥¼ í†µí•´ì„œ ìµœì ì˜ hyperparameter setì„ ì°¾ê³  hyperparametersë¥¼ ê³ ì •í•œ ìƒíƒœì—ì„œ í•™ìŠµì‹œí‚¬ ë•ŒëŠ” ëª¨ë“  ë°ì´í„°ë¥¼ ì‚¬ìš© â†’ ë” ë§ì€ ë°ì´í„°ë¥¼ ê°€ì§€ê³  í•™ìŠµí•˜ê³ ì!
- í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì€ ê·¸ ì–´ë– í•œ ê²½ìš°ì—ë„ ì‚¬ìš© ë¶ˆê°€ $\because$ cheating

## Bias and Variance

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%203.png)

- Variance: ë¹„ìŠ·í•œ ì…ë ¥ì— ëŒ€í•´ ì¶œë ¥ì´ ì–¼ë§ˆë‚˜ ì¼ê´€ì ìœ¼ë¡œ ë‚˜ì˜¤ëŠ”ê°€ë¥¼ ì¸¡ì •
- Bias: ì—¬ëŸ¬ ì…ë ¥ì— ëŒ€í•œ ì¶œë ¥ë“¤ì´ í‰ê· ì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ trueê°’ì— ê°€ê¹Œìš´ê°€ë¥¼ ì¸¡ì •

## Bias and Variance Trade-off

Given $\mathcal D=\{(x_i,t_i)\}_{i=1}^N$, where $t=f(x)+\epsilon$ and $\epsilon\sim\mathcal N(0,\sigma^2)$.

We can derive that what we are minimizing (cost) can be decomposed into three different parts: bias, variance, and noise.

$$
\begin{aligned}\underbrace{\mathbb E\left[(t-\hat f)^2\right]}_\text{cost}&=\mathbb E\left[(t-f+f-\hat f)^2\right]\\&=\cdots\\&=\underbrace{\mathbb E\left[\left(f-\mathbb E[\hat f]^2\right)^2\right]}_{\text{bias}^2}+\underbrace{\mathbb E\left[\left(\mathbb E[\hat f]-\hat f\right)^2\right]}_\text{variance}+\underbrace{\mathbb E[\epsilon]}_\text{noise}\end{aligned}
$$

- Bias, variance ë‘˜ ëª¨ë‘ë¥¼ ì¤„ì´ëŠ” ê²ƒì´ í˜ë“¤ë‹¤ëŠ” ê¸°ë³¸ì  ì œì•½

## Bootstrapping

- Bootstrapping is any test or metric that uses random sampling with replacement.
- í•™ìŠµ ë°ì´í„°ì˜ ì¼ë¶€ë¥¼ ì„ì˜ë¡œ êµ¬ì„±/í™œìš©(subsampling)í•œ ëª¨ë¸ì„ ì—¬ëŸ¿ ì‚¬ìš©í•œë‹¤ê³  í•  ë•Œ, ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ê°’ë“¤ì˜ consensusë¥¼ í™•ì¸í•˜ê³  ì „ì²´ ëª¨ë¸ì˜ uncertaintyë¥¼ í™•ì¸í•˜ê³ ì í•˜ëŠ” ê¸°ë²•

## Bagging vs. Boosting

- Bagging (Bootstrapping aggregating)
    - Multiple models are being trained with bootstrapping.
    - E.g., base classifiers are fitted on random subset where individual predictions are aggregated (voting or averaging).
    - ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì˜ votingì´ë‚˜ í‰ê· ì„ ì·¨í•˜ëŠ” ë°©ì‹ì´ ë‹¨ì¼ ëª¨ë¸ì˜ ì„±ëŠ¥ë³´ë‹¤ ì¢‹ì€ ê²½ìš°ê°€ ë§ìŒ
- Boosting
    - It focuses on those specific training samples that are hard to classify.
    - A strong model is built by combining weak learners in sequence where each learner learns from the mistakes of the previous weak learner.
    - Baseline ëª¨ë¸ì´ ì·¨ì•½í•œ ë°ì´í„°ì— ëŒ€í•´ì„œ ë³´ì™„ ëª¨ë¸ì„ ë§Œë“¤ê³ , ë˜ ì´ ëª¨ë¸ì´ ì·¨ì•½í•œ ë°ì´í„°ì— ëŒ€í•´ ë³´ì™„ ëª¨ë¸ì„ ë§Œë“¤ì–´ sequentialí•˜ê²Œ ëª¨ë¸ì„ ì´ì–´ robustnessë¥¼ í‚¤ìš°ëŠ” ë°©ì‹

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%204.png)

## Practical Gradient Descent Methods

### Gradient Descent Methods

- Stochastic gradient descent
    - Update with the gradient computed from a single sample.
- Mini-batch gradient descent
    - Update with the gradient computed form a subset of data.
- Batch gradient descent
    - Update with the gradient computed from the whole data.

### Batch-size matters

- â€œIt has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize.â€
- â€œWe â€¦ present numerical evidence that supports the view that large batch methods tend to converge to sharp minimizers of the training and testing functions. In contrast, small-batch methods consistently converge to flat minimizersâ€¦ this is due to the inherent noise in the gradient estimation.â€
- Batch-sizeë¥¼ ì‘ê²Œ ì„¤ì •í•˜ì—¬ sharp minimizersë³´ë‹¤ëŠ” flat minimizersì— ë„ë‹¬í•˜ëŠ” ê²ƒì´ ë‚«ë‹¤ëŠ” ì´ì•¼ê¸°
- On Large-batch Training for Deep Learning Generalization Gap and Sharp Minima, 2017. (ë…¼ë¬¸ ë¦¬ë”© ì¶”ì²œ)

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%205.png)

## Gradient Descent Methods

- SGD
- Momentum
- Nesterov accelerated gradient
- Adagrad
- Adadelta
- RMSprop
- Adam

### (Stochastic) Gradient Descent

$$
W_{t+1}\leftarrow W_t-\eta g_t
$$

- ë¬¸ì œ: learning rate (or step size) $\eta$ë¥¼ ì ì ˆíˆ ì¡ëŠ” ê²ƒì´ ë„ˆë¬´ ì–´ë ¤ì›€

### Momentum

$$
a_{t+1}\leftarrow\beta a_t+g_t\\W_{t+1}\leftarrow W_t-\eta a_{t+1}
$$

- ì´ì „ì˜ gradient ë°©í–¥ ì •ë³´ë¥¼ ë‹¤ìŒ gradientì—ë„ ë°˜ì˜í•˜ìëŠ” ì•„ì´ë””ì–´ â€” ê´€ì„±(momentum)
- í•´ì„
    - $\beta$: momentum
    - $a$: accumulation
- í•œ ë²ˆ í˜ëŸ¬ê°€ê¸° ì‹œì‘í•œ gradient pathë¥¼ ì–´ëŠ ì •ë„ ìœ ì§€ì‹œì¼œì¤Œ â†’ mini-batch gradientì˜ randomnessê°€ ìˆë‹¤ê³  í•˜ë”ë¼ë„ í•™ìŠµ íš¨ê³¼ë¥¼ í–¥ìƒ ì‹œì¼œì¤Œ.

### Nesterov Accelerated Gradient (NAG)

$$
a_{t+1}\leftarrow\beta a_t+\nabla\mathcal L(W_t-\eta\beta a_t)\\W_{t+1}\leftarrow W_t-\eta a_{t+1}
$$

- í•´ì„
    - $\nabla\mathcal L(W_t-\eta\beta a_t)$: lookahead gradient
- ê°€ì¥ í° ì¥ì : ë¹„êµì  ë¹ ë¥´ê²Œ local minimaë¡œ ìˆ˜ë ´
    - ì´ë¡ ì ìœ¼ë¡œë„ momentumì— ë¹„í•´ ë†’ì€ converge ratioë¥¼ ê°€ì§ì„ ì¦ëª… ê°€ëŠ¥
- Momentum ë°©ì‹ê³¼ ë‹¬ë¦¬, $a$ ë°©í–¥ìœ¼ë¡œ ì´ë™í•œ ë’¤ gradientë¥¼ ê³„ì‚°í•˜ì—¬ ì´ë™

### Adagrad

$$
W_{t+1}\leftarrow W_t-\frac{\eta}{\sqrt{G_t+\epsilon}}g_t
$$

- Adagrad adapts the learning rate, performing larger updates for infrequent and smaller updates for frequent parameters.
- ì‹ ê²½ë§ì˜ íŒŒë¼ë¯¸í„°ê°€ í˜„ì¬ê¹Œì§€ ì–¼ë§ˆë‚˜ ë³€í•´ì™”ëŠ”ì§€, ì•ˆ ë³€í–ˆëŠ”ì§€ë¥¼ ì‚´í•Œ â€” í¬ê²Œ ë³€í™”í•´ ì˜¨ íŒŒë¼ë¯¸í„°ëŠ” ë” ì ê²Œ, ì ê²Œ ë³€í™”í•´ ì˜¨ íŒŒë¼ë¯¸í„°ëŠ” ë” í¬ê²Œ ë³€í™”ì‹œí‚¤ë ¤ê³  í•¨
- í•´ì„
    - $G_t$: sum of gradient squares
    - $\epsilon$ ~ for numerical stability
- ë¬¸ì œ
    - $G_t$ê°€ ì§€ì†ì ìœ¼ë¡œ ì»¤ì§ì— ë”°ë¼ ê¸´ ì‹œê°„ í•™ìŠµ ì‹œ í•™ìŠµì´ ë©ˆì¶°ì§€ëŠ” í˜„ìƒ ë°œìƒ
    - What will happen if the training occurs for a long period?

### Adadelta

$$
G_t=\gamma G_{t-1}+(1-\gamma)g_t^2\\W_{t+1}=W_t-\frac{\sqrt{H_{t-1}+\epsilon}}{\sqrt{G_t+\epsilon}}g_t\\H_t=\gamma H_{t-1}+(1-\gamma)(\Delta W_t)^2
$$

- Adadelta extends Adagrad to reduce its monotonically decreasing the learning rate by restricting the accumulation window.
- Adagradì˜ ë¬¸ì œë¥¼ í•´ê²°í•  ì‰¬ìš´ ì•„ì´ë””ì–´: í˜„ì¬ timestep $t$ì—ì„œ ì–´ëŠ ì •ë„ window sizeë§Œí¼ gradientì˜ ì œê³±ì˜ ë³€í™”ì— ì–¼ë§ˆë‚˜ ë¯¼ê°í•œì§€ í™•ì¸
- í•˜ì§€ë§Œ ì´ì™€ ê°™ì€ ë°©ì‹ì€ window sizeì— ë”°ë¼ ì´ì „ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ë³´ì¡´í•˜ê³  ìˆì–´ì•¼ í•˜ëŠ”ì§€ê°€ ë‹¬ë¼ì§€ëŠ”ë°, GPT-3ì™€ ê°™ì´ $G_t$ê°€ 1000ì–µ ì—¬ê°œ ì´ìƒì˜ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ê°€ì§€ëŠ”ë° ì´ë¥¼ ìˆ˜ window sizeë§Œí¼ì˜ ë°°ìˆ˜ì˜ ì •ë³´ë¥¼ ë³´ì¡´í•˜ê³  ìˆëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥
- ì´ë¥¼ ë§‰ì„ ìˆ˜ ìˆëŠ” ë°©ë²•?
    
    â†’ exponential moving average (EMA)
    
- íŠ¹ì§•: learning rate ì¡´ì¬ x â†’ ë°”ê¿€ ìˆ˜ ìˆëŠ” ë¶€ë¶„ì´ ì ì–´ ë§ì´ í™œìš©í•˜ì§€ ì•ŠìŒ
- í•´ì„
    - $G_t$: EMA of gradient squares
    - $H_t$: EMA of difference squares

### RMSprop

- RMSprop is an unpublished, adaptive learning rate method proposed by Geoffrey Hinton in his lecture.

$$
G_t=\gamma G_{t-1}+(1-\gamma)g_t^2\\W_{t+1}=W_t-\frac{\eta}{\sqrt{G_t+\epsilon}}g_t
$$

- í•´ì„
    - $G_t$: EMA of gradient squares
    - $\eta$: step size

### Adam

$$
m_t=\beta_1m_{t=1}+(1-\beta_1)g_t\\v_t=\beta_2v_{t-1}+(1-\beta_2)g_t^2\\W_{t+1}=W_t-\frac{\eta}{\sqrt{v_t+\epsilon}}\frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t}m_t
$$

- Adaptive Momenet Estimation (Adam) leverages both past gradients and squared gradients.
- ì¼ë°˜ì ìœ¼ë¡œ ì˜ ë˜ê³  ê°€ì¥ ë¬´ë‚œí•˜ê²Œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹
- Momentum + adaptive learning rate approach
- í•´ì„
    - $m_t$: momentum
    - $v_t$: EMA of gradient squares
    - $\eta$: step size
- Tip: $\epsilon=10^{-7}$ì„ ì˜ ë°”ê¾¸ì–´ ì£¼ëŠ” ê²ƒì´ ì¤‘ìš” in practice

## Regularization

- Early stopping
- Parameter norm penalty
- Data augmentation
- Noise robustness
- Label smoothing
- Dropout
- Batch normalization

### Early stopping

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%206.png)

### Parameter norm penalty

- This adds smoothness to the function space.

$$
\text{total cost}=\text{loss}(\mathcal D;W)+\frac{\alpha}{2}\Vert W\Vert_2^2
$$

- Weight decayë¼ê³ ë„ í•¨
- í•´ì„
    - ì‹ ê²½ë§ì´ ë§Œë“¤ì–´ë‚´ëŠ” function spaceì—ì„œ ì´ í•¨ìˆ˜ë¥¼ ìµœëŒ€í•œ ë¶€ë“œëŸ¬ìš´(smooth) í•¨ìˆ˜ë¡œ ë³´ìëŠ” ê²ƒ
    - Why? ë¶€ë“œëŸ¬ìš´ í•¨ìˆ˜(smooth function)ì¼ìˆ˜ë¡ ì¼ë°˜í™” ì„±ëŠ¥(generalization performance)ì´ ì¢‹ì„ ê²ƒ

### Data augmentation

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%207.png)

- ë°ì´í„°ê°€ ë¬´í•œíˆ ë§ìœ¼ë©´, ì›¬ë§Œí•˜ë©´ ì˜ ë¨
- ë°ì´í„°ê°€ ì ì€ ê²½ìš°, ë°ì´í„° ì¦ëŒ€(data augmentation) í•„ìš”ã…ˆ

### Noise robustness

- ì…ë ¥ ë˜ëŠ” íŒŒë¼ë¯¸í„°ì— random noiseë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒ
- ì™œ ì˜ ë˜ëŠ”ê°€ì— ëŒ€í•´ì„œëŠ” ì•„ì§ê¹Œì§€ ì˜ë¬¸ì´ ì¡´ì¬

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%208.png)

### Label smoothing

- Mix-up constructs augmented training examples by mixing both input and output of two randomly selected training data.
    - Decision boundary ë¶€ë“œëŸ½ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” íš¨ê³¼
    
    ![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%209.png)
    
- CutMix constructs augmented training examples by mixing inputs with cut and paste and outputs with soft labels of two randomly selected training data.
    
    ![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%2010.png)
    
    - ì„±ëŠ¥ì´ êµ‰ì¥íˆ ë§ì´ ì˜¬ë¼ê°!
    - ë“¤ì´ëŠ” ë…¸ë ¥ ëŒ€ë¹„ ì„±ëŠ¥ì„ ë§ì´ ì˜¬ë¦´ ìˆ˜ ìˆëŠ” ë°©ë²•

### Dropout

- In each forward pass, randomly set some neurons to zero.

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%2011.png)

- í•´ì„
    - ê°ê°ì˜ neuronë“¤ë¡œë¶€í„° ì¶”ì¶œë˜ëŠ” robust featuresê°€ ë‚¨ìŒ
    - ìˆ˜í•™ì ìœ¼ë¡œ ì—„ë°€íˆ ì¦ëª…ëœ ê²ƒì€ ì•„ë‹˜

### Batch normalization

- Batch normalization compute the empirical mean and variance independently for each dimension (layers) and normalize.
- ë…¼ë€ì´ ìˆëŠ” ë¶€ë¶„: internal covariate shift

$$
\begin{aligned}\mu_B&=\frac{1}{m}\sum_{i=1}^mx_i\\\sigma_B^2&=\frac{1}{m}\sum_{i=1}^m(x_i-\mu_B)^2\\\hat x_i&=\frac{x_i-\mu_B}{\sqrt{\sigma_B^2+\epsilon}}\end{aligned}
$$

- ì‹ ê²½ë§ ì¸µì„ ë§ì´ ìŒ“ì„ ë•Œ BNì„ í™œìš©í•˜ë©´ ì„±ëŠ¥ì´ ì˜¬ë¼ê°
- ë…¼ë¬¸ì˜ í•´ì„
    - í‰ê· ì„ 0ìœ¼ë¡œ, ë¶„ì‚°ì„ 1ë¡œ ë§Œë“¤ì–´ì¤Œìœ¼ë¡œì¨ internal covariate (feature) shiftê°€ ì¤„ì–´ë“ ë‹¤
    - ì´í›„ ë…¼ë¬¸ë“¤ì€ ì´ì— ëŒ€í•´ ë°˜ë°•

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%2012.png)

### ë§ˆë¬´ë¦¬

- ë„¤íŠ¸ì›Œí¬ë¥¼ ë§Œë“¤ê³ , ë°ì´í„°ì™€ ë¬¸ì œ ë“±ì´ ê³ ì •ë˜ì–´ ìˆì„ ë•Œ, ì´ëŸ¬í•œ ì •ê·œí™”(regularization) ê¸°ë²•ì„ ì‚¬ìš©í•´ë³´ë©´ì„œ ì¼ë°˜í™” ì„±ëŠ¥(generalization performance)ì„ ì‚´í•Œ

# Convolutional Neural Networks

## Convolution

### ì •ì˜

- Signal processing ë¶„ì•¼ì—ì„œ ë‘ í•¨ìˆ˜ë¥¼ ì ì ˆíˆ ì„ì–´ì£¼ëŠ” ë°©ë²•/operator
- Continuous convolution
    
    $$
    (f*g)(t)=\int f(\tau)g(t-\tau)d\tau=\int f(t-\tau)g(t)d\tau
    $$
    
- Discrete convolution
    
    $$
    (f*g)(t)=\sum_{i=-\infty}^\infty f(i)g(t-i)=\sum_{i=-\infty}^\infty f(t-i)g(i)
    $$
    
- 2D image convolution
    
    $$
    \begin{aligned}(I*K)(i,j)&=\sum_m\sum_nI(m,n)K(i-m,j-n)\\&=\sum_m\sum_nI(i-m,i-n)K(m,n)\end{aligned}
    $$
    

### 2D convolutionì˜ ì˜ë¯¸?

- í•´ë‹¹ convolution filterì˜ ëª¨ì–‘ì„ ì´ë¯¸ì§€ì— ì°ëŠ” ê²ƒ

### RGB Image Convolution

- Input channelê³¼ output convolution feature mapì˜ channelì„ ì•Œë©´ ì ìš©ëœ convolution featureì˜ í¬ê¸°ë¥¼ ì•Œ ìˆ˜ ìˆìŒ

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%2013.png)

### Stack of Convolutions

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%2014.png)

## Convolutional Neural Networks

- CNN consists of convolution layer, pooling layer, and fully-connected layer.
    - Convolution and pooling layers: feature extraction
    - Fully-connected layer: decision making (e.g., classification)
    - ìµœê·¼ FC layerê°€ ìµœì†Œí™” ë‚´ì§€ëŠ” ì—†ì–´ì§€ëŠ” ì¶”ì„¸
        
        â†’ íŒŒë¼ë¯¸í„° ìˆ˜ ë•Œë¬¸
        
    - í•™ìŠµí•˜ê³ ì í•˜ëŠ” íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ í•™ìŠµì´ ì–´ë µê³ , ì¼ë°˜í™” ì„±ëŠ¥(generalization performance) ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìŒ
        - ì¼ë°˜í™” ì„±ëŠ¥ì´ë€?
            
            í•™ìŠµì—ì„œ ì–»ì–´ì§„ ê²°ê³¼ê°€ í•œ ë²ˆë„ ë³¸ ì ì´ ì—†ëŠ” ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì–¼ë§ˆë‚˜ ì˜ ì‘ë™í•˜ëŠ”ì§€ì˜ ì²™ë„
            
    - CNN ë°œì „ ë°©í–¥
        
        ê°™ì€ ëª¨ë¸ì„ ë§Œë“¤ê³  ìµœëŒ€í•œ ëª¨ë¸ì„ ê¹Šê²Œ ìŒ“ìœ¼ë©´ì„œ ë™ì‹œì— íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì¤„ì´ëŠ” ë° ì§‘ì¤‘
        
    - ê° layerë§ˆë‹¤ì˜ íŒŒë¼ë¯¸í„° ìˆ˜ì™€ ëª¨ë¸ ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜ì— ëŒ€í•œ ê°ì„ ê°€ì§€ëŠ” ê²ƒì´ ì¤‘ìš”

## Convolution Arithmetic of GoogLeNet

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%2015.png)

### Stride & Padding

- Stride: filterë¥¼ ëª‡ í”½ì…€ ë‹¨ìœ„ë¡œ ì´ë¯¸ì§€ì— ì°ì„ ê²ƒì¸ì§€
- Padding: boundary ì •ë³´ë¥¼ ì–´ëŠ ì •ë„ì˜ í”½ì…€ ë‘ê»˜ë¡œ ë³´ì¡´í•  ê²ƒì¸ì§€
    - no padding, zero padding

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%2016.png)

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%2017.png)

- ê° ì¸µì—ì„œì˜ íŒŒë¼ë¯¸í„° ìˆ˜ì˜ orderì— ëŒ€í•œ ê°ì´ ìˆëŠ” ê²ƒì´ ì¤‘ìš”

### AlexNet íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%2018.png)

- ì²«ë²ˆì§¸ ì¸µì„ í†µê³¼í•œ ë’¤ 96 ì±„ë„ì§œë¦¬ feature mapì„ êµ¬ì„±í•´ì•¼ í–ˆìœ¼ë‚˜, ë‹¹ì‹œ GPU ë©”ëª¨ë¦¬ì˜ í•œê³„ë¥¼ ì´ìœ ë¡œ 48ì±„ë„ì§œë¦¬ feature map 2ê°œë¥¼ ì“°ë„ë¡ íŒŒíŠ¸ë¥¼ ë‚˜ëˆ„ê²Œ ë¨
- ì²«ë²ˆì§¸ ì¸µ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° â†’ 35k

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%2019.png)

- ë‘ë²ˆì§¸ ì¸µ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° â†’ 307k

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%2020.png)

- ì„¸ë²ˆì§¸ ì¸µ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° â†’ 884k

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%2021.png)

- ë„¤ë²ˆì§¸ ì¸µ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° â†’ 663k

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%2022.png)

- ë‹¤ì„¯ë²ˆì§¸ ì¸µ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° â†’ 442k

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%2023.png)

- FC ì¸µ (1) íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° â†’ 177M

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%2024.png)

- FC ì¸µ (2) íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° â†’ 16M

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%2025.png)

- FC ì¸µ (3) íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° â†’ 4M
- Dense ì¸µì´ ì¼ë°˜ì ìœ¼ë¡œ í›¨ì”¬ ë” ë§ì€ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ê°€ì§€ê²Œ ë˜ëŠ” ì´ìœ ?
    
    Convolution operatorì™€ ê°ê°ì˜ ì»¤ë„ì´ ëª¨ë“  ìœ„ì¹˜ì— ëŒ€í•´ ë™ì¼í•˜ê²Œ ì ìš©ë˜ê¸° ë•Œë¬¸
    
- ê²°êµ­ ëŒ€ë¶€ë¶„ì˜ íŒŒë¼ë¯¸í„°ê°€ FC ì¸µì— ì§‘ì¤‘ë˜ë‹¤ ë³´ë‹ˆ ë„¤íŠ¸ì›Œí¬ì˜ ë°œì „ ë°©í–¥ì´ FC ë¶€ë¶„ì„ ìµœëŒ€í•œìœ¼ë¡œ ì¤„ì´ê³  ì•ë‹¨ì˜ convolution ì¸µì„ ê¹Šê²Œ ìŒ“ëŠ” ê²ƒì´ íŠ¸ë Œë“œê°€ ë¨

## 1 x 1 Convolution

![Untitled](DL%20Basics%20by%20%E1%84%8E%E1%85%AC%E1%84%89%E1%85%A5%E1%86%BC%E1%84%8C%E1%85%AE%E1%86%AB%20(2023-03-20%20~%202023-03-24)%20094a27599aab45a9bafd81fa7b752988/Untitled%2026.png)

- Why?
    - Dimension reduction
    - To reduce the number of parameters while increasing the depth
    - e.g., bottleneck architecture

# Modern CNN

# Computer Vision Applications

# Recurrent Neural Networks

# Transformer

# Generative Models Part 1

# Generative Models Part 2

---