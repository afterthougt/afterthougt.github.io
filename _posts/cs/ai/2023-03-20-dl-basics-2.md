---
layout: post
title: ìµœì í™”
category: cs
tags: ai
---

# [Deep Learning/ë”¥ ëŸ¬ë‹] ìµœì í™” ê¸°ë²•

- 30ë¶„ ë‚´ ë‹´ê¸°ì— ë„ˆë¬´ ë°©ëŒ€í•œ ì£¼ì œ
- ê°ê°ì˜ ë¶„ì•¼ë§ˆë‹¤ ìµœì†Œ í•œ í•™ê¸° ì´ìƒ ê³µë¶€í•˜ëŠ” ë¶„ì•¼
- ì¤„ì´ê³  ì¤„ì—¬ì„œ ì‹¤ì œë¡œ ì ìš© ì‹œ ì¤‘ìš”í•˜ê²Œ ì•Œì•„ë‘ì–´ì•¼ í•  ê°œë…ë“¤ ìœ„ì£¼ë¡œ

## Introduction

<aside>
ğŸ’¡ â€œLanguage is the source of misunderstanding,â€ Antoine de Saint-ExupÃ¨ry (1900~1944)

</aside>

### Gradient Descent

- 1st-order iterative optimization algorithm for finding a local minimum of a differentiable function.

### Important Concepts in Optimization

- Generalization
- Underfitting vs. overfitting
- Cross-validation
- Bias-variance trade-off
- Bootstrapping
- Bagging and boosting

## Generalization

- How well the learned model will behave on unseen data.

![2023-03-20-dl-basics-2-fig1](../../../assets/img/2023-03-20-dl-basics-2-fig1.png)

## Underfitting vs. Overfitting

![2023-03-20-dl-basics-2-fig2](../../../assets/img/2023-03-20-dl-basics-2-fig2.png)

## Cross-validation

- Cross-validation is a model validation technique for assessing how the model will generalize to an independent (test) dataset.

![2023-03-20-dl-basics-2-fig3](../../../assets/img/2023-03-20-dl-basics-2-fig3.png)

- CVë¥¼ í†µí•´ì„œ ìµœì ì˜ hyperparameter setì„ ì°¾ê³  hyperparametersë¥¼ ê³ ì •í•œ ìƒíƒœì—ì„œ í•™ìŠµì‹œí‚¬ ë•ŒëŠ” ëª¨ë“  ë°ì´í„°ë¥¼ ì‚¬ìš© â†’ ë” ë§ì€ ë°ì´í„°ë¥¼ ê°€ì§€ê³  í•™ìŠµí•˜ê³ ì!
- í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì€ ê·¸ ì–´ë– í•œ ê²½ìš°ì—ë„ ì‚¬ìš© ë¶ˆê°€ $$\because$$ cheating

## Bias and Variance

![2023-03-20-dl-basics-2-fig4](../../../assets/img/2023-03-20-dl-basics-2-fig4.png)
- Variance: ë¹„ìŠ·í•œ ì…ë ¥ì— ëŒ€í•´ ì¶œë ¥ì´ ì–¼ë§ˆë‚˜ ì¼ê´€ì ìœ¼ë¡œ ë‚˜ì˜¤ëŠ”ê°€ë¥¼ ì¸¡ì •
- Bias: ì—¬ëŸ¬ ì…ë ¥ì— ëŒ€í•œ ì¶œë ¥ë“¤ì´ í‰ê· ì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ trueê°’ì— ê°€ê¹Œìš´ê°€ë¥¼ ì¸¡ì •

## Bias and Variance Trade-off

Given $$\mathcal D=\{(x_i,t_i)\}_{i=1}^N$$, where $$t=f(x)+\epsilon$$ and $$\epsilon\sim\mathcal N(0,\sigma^2)$$.

We can derive that what we are minimizing (cost) can be decomposed into three different parts: bias, variance, and noise.

$$
\begin{aligned}\underbrace{\mathbb E\left[(t-\hat f)^2\right]}_\text{cost}&=\mathbb E\left[(t-f+f-\hat f)^2\right]\\&=\cdots\\&=\underbrace{\mathbb E\left[\left(f-\mathbb E[\hat f]^2\right)^2\right]}_{\text{bias}^2}+\underbrace{\mathbb E\left[\left(\mathbb E[\hat f]-\hat f\right)^2\right]}_\text{variance}+\underbrace{\mathbb E[\epsilon]}_\text{noise}\end{aligned}
$$

- Bias, variance ë‘˜ ëª¨ë‘ë¥¼ ì¤„ì´ëŠ” ê²ƒì´ í˜ë“¤ë‹¤ëŠ” ê¸°ë³¸ì  ì œì•½

## Bootstrapping

- Bootstrapping is any test or metric that uses random sampling with replacement.
- í•™ìŠµ ë°ì´í„°ì˜ ì¼ë¶€ë¥¼ ì„ì˜ë¡œ êµ¬ì„±/í™œìš©(subsampling)í•œ ëª¨ë¸ì„ ì—¬ëŸ¿ ì‚¬ìš©í•œë‹¤ê³  í•  ë•Œ, ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ê°’ë“¤ì˜ consensusë¥¼ í™•ì¸í•˜ê³  ì „ì²´ ëª¨ë¸ì˜ uncertaintyë¥¼ í™•ì¸í•˜ê³ ì í•˜ëŠ” ê¸°ë²•

## Bagging vs. Boosting

- Bagging (Bootstrapping aggregating)
    - Multiple models are being trained with bootstrapping.
    - E.g., base classifiers are fitted on random subset where individual predictions are aggregated (voting or averaging).
    - ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì˜ votingì´ë‚˜ í‰ê· ì„ ì·¨í•˜ëŠ” ë°©ì‹ì´ ë‹¨ì¼ ëª¨ë¸ì˜ ì„±ëŠ¥ë³´ë‹¤ ì¢‹ì€ ê²½ìš°ê°€ ë§ìŒ
- Boosting
    - It focuses on those specific training samples that are hard to classify.
    - A strong model is built by combining weak learners in sequence where each learner learns from the mistakes of the previous weak learner.
    - Baseline ëª¨ë¸ì´ ì·¨ì•½í•œ ë°ì´í„°ì— ëŒ€í•´ì„œ ë³´ì™„ ëª¨ë¸ì„ ë§Œë“¤ê³ , ë˜ ì´ ëª¨ë¸ì´ ì·¨ì•½í•œ ë°ì´í„°ì— ëŒ€í•´ ë³´ì™„ ëª¨ë¸ì„ ë§Œë“¤ì–´ sequentialí•˜ê²Œ ëª¨ë¸ì„ ì´ì–´ robustnessë¥¼ í‚¤ìš°ëŠ” ë°©ì‹

![2023-03-20-dl-basics-2-fig5](../../../assets/img/2023-03-20-dl-basics-2-fig5.png)

## Practical Gradient Descent Methods

### Gradient Descent Methods

- Stochastic gradient descent
    - Update with the gradient computed from a single sample.
- Mini-batch gradient descent
    - Update with the gradient computed form a subset of data.
- Batch gradient descent
    - Update with the gradient computed from the whole data.

### Batch-size matters

- â€œIt has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize.â€
- â€œWe â€¦ present numerical evidence that supports the view that large batch methods tend to converge to sharp minimizers of the training and testing functions. In contrast, small-batch methods consistently converge to flat minimizersâ€¦ this is due to the inherent noise in the gradient estimation.â€
- Batch-sizeë¥¼ ì‘ê²Œ ì„¤ì •í•˜ì—¬ sharp minimizersë³´ë‹¤ëŠ” flat minimizersì— ë„ë‹¬í•˜ëŠ” ê²ƒì´ ë‚«ë‹¤ëŠ” ì´ì•¼ê¸°
- On Large-batch Training for Deep Learning Generalization Gap and Sharp Minima, 2017. (ë…¼ë¬¸ ë¦¬ë”© ì¶”ì²œ)

![2023-03-20-dl-basics-2-fig6](../../../assets/img/2023-03-20-dl-basics-2-fig6.png)

## Gradient Descent Methods

- SGD
- Momentum
- Nesterov accelerated gradient
- Adagrad
- Adadelta
- RMSprop
- Adam

### (Stochastic) Gradient Descent

$$
W_{t+1}\leftarrow W_t-\eta g_t
$$

- ë¬¸ì œ: learning rate (or step size) $$\eta$$ë¥¼ ì ì ˆíˆ ì¡ëŠ” ê²ƒì´ ë„ˆë¬´ ì–´ë ¤ì›€

### Momentum

$$
\begin{aligned}
a_{t+1}&\leftarrow\beta a_t+g_t\\W_{t+1}&\leftarrow W_t-\eta a_{t+1}
\end{aligned}
$$

- ì´ì „ì˜ gradient ë°©í–¥ ì •ë³´ë¥¼ ë‹¤ìŒ gradientì—ë„ ë°˜ì˜í•˜ìëŠ” ì•„ì´ë””ì–´ â€” ê´€ì„±(momentum)
- í•´ì„
  - $$
    \beta
    $$: momentum
  - $$
    a
    $$: accumulation
- í•œ ë²ˆ í˜ëŸ¬ê°€ê¸° ì‹œì‘í•œ gradient pathë¥¼ ì–´ëŠ ì •ë„ ìœ ì§€ì‹œì¼œì¤Œ â†’ mini-batch gradientì˜ randomnessê°€ ìˆë‹¤ê³  í•˜ë”ë¼ë„ í•™ìŠµ íš¨ê³¼ë¥¼ í–¥ìƒ ì‹œì¼œì¤Œ.

### Nesterov Accelerated Gradient (NAG)

$$
\begin{aligned}
a_{t+1}&\leftarrow\beta a_t+\nabla\mathcal L(W_t-\eta\beta a_t)\\W_{t+1}&\leftarrow W_t-\eta a_{t+1}
\end{aligned}
$$

- í•´ì„
    - $$
      \nabla\mathcal L(W_t-\eta\beta a_t)
      $$: lookahead gradient
- ê°€ì¥ í° ì¥ì : ë¹„êµì  ë¹ ë¥´ê²Œ local minimaë¡œ ìˆ˜ë ´
    - ì´ë¡ ì ìœ¼ë¡œë„ momentumì— ë¹„í•´ ë†’ì€ converge ratioë¥¼ ê°€ì§ì„ ì¦ëª… ê°€ëŠ¥
- Momentum ë°©ì‹ê³¼ ë‹¬ë¦¬, $$a$$ ë°©í–¥ìœ¼ë¡œ ì´ë™í•œ ë’¤ gradientë¥¼ ê³„ì‚°í•˜ì—¬ ì´ë™

### Adagrad

$$
W_{t+1}\leftarrow W_t-\frac{\eta}{\sqrt{G_t+\epsilon}}g_t
$$

- Adagrad adapts the learning rate, performing larger updates for infrequent and smaller updates for frequent parameters.
- ì‹ ê²½ë§ì˜ íŒŒë¼ë¯¸í„°ê°€ í˜„ì¬ê¹Œì§€ ì–¼ë§ˆë‚˜ ë³€í•´ì™”ëŠ”ì§€, ì•ˆ ë³€í–ˆëŠ”ì§€ë¥¼ ì‚´í•Œ â€” í¬ê²Œ ë³€í™”í•´ ì˜¨ íŒŒë¼ë¯¸í„°ëŠ” ë” ì ê²Œ, ì ê²Œ ë³€í™”í•´ ì˜¨ íŒŒë¼ë¯¸í„°ëŠ” ë” í¬ê²Œ ë³€í™”ì‹œí‚¤ë ¤ê³  í•¨
- í•´ì„
    - $$
      G_t
      $$: sum of gradient squares
    - $$
      \epsilon
      $$ ~ for numerical stability
- ë¬¸ì œ
    - $$
      G_t
      $$ê°€ ì§€ì†ì ìœ¼ë¡œ ì»¤ì§ì— ë”°ë¼ ê¸´ ì‹œê°„ í•™ìŠµ ì‹œ í•™ìŠµì´ ë©ˆì¶°ì§€ëŠ” í˜„ìƒ ë°œìƒ
    - What will happen if the training occurs for a long period?

### Adadelta

$$
\begin{aligned}
G_t&=\gamma G_{t-1}+(1-\gamma)g_t^2\\W_{t+1}&=W_t-\frac{\sqrt{H_{t-1}+\epsilon}}{\sqrt{G_t+\epsilon}}g_t\\H_t&=\gamma H_{t-1}+(1-\gamma)(\Delta W_t)^2
\end{aligned}
$$

- Adadelta extends Adagrad to reduce its monotonically decreasing the learning rate by restricting the accumulation window.
- Adagradì˜ ë¬¸ì œë¥¼ í•´ê²°í•  ì‰¬ìš´ ì•„ì´ë””ì–´: í˜„ì¬ timestep $$t$$ì—ì„œ ì–´ëŠ ì •ë„ window sizeë§Œí¼ gradientì˜ ì œê³±ì˜ ë³€í™”ì— ì–¼ë§ˆë‚˜ ë¯¼ê°í•œì§€ í™•ì¸
- í•˜ì§€ë§Œ ì´ì™€ ê°™ì€ ë°©ì‹ì€ window sizeì— ë”°ë¼ ì´ì „ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ë³´ì¡´í•˜ê³  ìˆì–´ì•¼ í•˜ëŠ”ì§€ê°€ ë‹¬ë¼ì§€ëŠ”ë°, GPT-3ì™€ ê°™ì´ $$G_t$$ê°€ 1000ì–µ ì—¬ê°œ ì´ìƒì˜ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ê°€ì§€ëŠ”ë° ì´ë¥¼ ìˆ˜ window sizeë§Œí¼ì˜ ë°°ìˆ˜ì˜ ì •ë³´ë¥¼ ë³´ì¡´í•˜ê³  ìˆëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥
- ì´ë¥¼ ë§‰ì„ ìˆ˜ ìˆëŠ” ë°©ë²•?
    
    â†’ exponential moving average (EMA)
    
- íŠ¹ì§•: learning rate ì¡´ì¬ x â†’ ë°”ê¿€ ìˆ˜ ìˆëŠ” ë¶€ë¶„ì´ ì ì–´ ë§ì´ í™œìš©í•˜ì§€ ì•ŠìŒ
- í•´ì„
    - $$
      G_t
      $$: EMA of gradient squares
    - $$
      H_t
      $$: EMA of difference squares

### RMSprop

- RMSprop is an unpublished, adaptive learning rate method proposed by Geoffrey Hinton in his lecture.

$$
\begin{aligned}
G_t&=\gamma G_{t-1}+(1-\gamma)g_t^2\\W_{t+1}&=W_t-\frac{\eta}{\sqrt{G_t+\epsilon}}g_t
\end{aligned}
$$

- í•´ì„
    - $$
      G_t
      $$: EMA of gradient squares
    - $$
      \eta
      $$: step size

### Adam

$$
\begin{aligned}
m_t&=\beta_1m_{t=1}+(1-\beta_1)g_t\\v_t&=\beta_2v_{t-1}+(1-\beta_2)g_t^2\\W_{t+1}&=W_t-\frac{\eta}{\sqrt{v_t+\epsilon}}\frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t}m_t
\end{aligned}
$$

- Adaptive Momenet Estimation (Adam) leverages both past gradients and squared gradients.
- ì¼ë°˜ì ìœ¼ë¡œ ì˜ ë˜ê³  ê°€ì¥ ë¬´ë‚œí•˜ê²Œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹
- Momentum + adaptive learning rate approach
- í•´ì„
    - $$
      m_t
      $$: momentum
    - $$
      v_t
      $$: EMA of gradient squares
    - $$
      \eta
      $$: step size
- Tip: $$\epsilon=10^{-7}$$ì„ ì˜ ë°”ê¾¸ì–´ ì£¼ëŠ” ê²ƒì´ ì¤‘ìš” in practice

## Regularization

- Early stopping
- Parameter norm penalty
- Data augmentation
- Noise robustness
- Label smoothing
- Dropout
- Batch normalization

### Early stopping

![2023-03-20-dl-basics-2-fig7](../../../assets/img/2023-03-20-dl-basics-2-fig7.png)

### Parameter norm penalty

- This adds smoothness to the function space.

$$
\text{total cost}=\text{loss}(\mathcal D;W)+\frac{\alpha}{2}\Vert W\Vert_2^2
$$

- Weight decayë¼ê³ ë„ í•¨
- í•´ì„
    - ì‹ ê²½ë§ì´ ë§Œë“¤ì–´ë‚´ëŠ” function spaceì—ì„œ ì´ í•¨ìˆ˜ë¥¼ ìµœëŒ€í•œ ë¶€ë“œëŸ¬ìš´(smooth) í•¨ìˆ˜ë¡œ ë³´ìëŠ” ê²ƒ
    - Why? ë¶€ë“œëŸ¬ìš´ í•¨ìˆ˜(smooth function)ì¼ìˆ˜ë¡ ì¼ë°˜í™” ì„±ëŠ¥(generalization performance)ì´ ì¢‹ì„ ê²ƒ

### Data augmentation

![2023-03-20-dl-basics-2-fig8](../../../assets/img/2023-03-20-dl-basics-2-fig8.png)

- ë°ì´í„°ê°€ ë¬´í•œíˆ ë§ìœ¼ë©´, ì›¬ë§Œí•˜ë©´ ì˜ ë¨
- ë°ì´í„°ê°€ ì ì€ ê²½ìš°, ë°ì´í„° ì¦ëŒ€(data augmentation) í•„ìš”ã…ˆ

### Noise robustness

- ì…ë ¥ ë˜ëŠ” íŒŒë¼ë¯¸í„°ì— random noiseë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒ
- ì™œ ì˜ ë˜ëŠ”ê°€ì— ëŒ€í•´ì„œëŠ” ì•„ì§ê¹Œì§€ ì˜ë¬¸ì´ ì¡´ì¬

![2023-03-20-dl-basics-2-fig9](../../../assets/img/2023-03-20-dl-basics-2-fig9.png)

### Label smoothing

- Mix-up constructs augmented training examples by mixing both input and output of two randomly selected training data.
    - Decision boundary ë¶€ë“œëŸ½ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” íš¨ê³¼
    
    ![2023-03-20-dl-basics-2-fig10](../../../assets/img/2023-03-20-dl-basics-2-fig10.png)
    
- CutMix constructs augmented training examples by mixing inputs with cut and paste and outputs with soft labels of two randomly selected training data.
    
    ![2023-03-20-dl-basics-2-fig11](../../../assets/img/2023-03-20-dl-basics-2-fig11.png)
    
    - ì„±ëŠ¥ì´ êµ‰ì¥íˆ ë§ì´ ì˜¬ë¼ê°!
    - ë“¤ì´ëŠ” ë…¸ë ¥ ëŒ€ë¹„ ì„±ëŠ¥ì„ ë§ì´ ì˜¬ë¦´ ìˆ˜ ìˆëŠ” ë°©ë²•

### Dropout

- In each forward pass, randomly set some neurons to zero.

![2023-03-20-dl-basics-2-fig12](../../../assets/img/2023-03-20-dl-basics-2-fig12.png)

- í•´ì„
    - ê°ê°ì˜ neuronë“¤ë¡œë¶€í„° ì¶”ì¶œë˜ëŠ” robust featuresê°€ ë‚¨ìŒ
    - ìˆ˜í•™ì ìœ¼ë¡œ ì—„ë°€íˆ ì¦ëª…ëœ ê²ƒì€ ì•„ë‹˜

### Batch normalization

- Batch normalization compute the empirical mean and variance independently for each dimension (layers) and normalize.
- ë…¼ë€ì´ ìˆëŠ” ë¶€ë¶„: internal covariate shift

$$
\begin{aligned}\mu_B&=\frac{1}{m}\sum_{i=1}^mx_i\\\sigma_B^2&=\frac{1}{m}\sum_{i=1}^m(x_i-\mu_B)^2\\\hat x_i&=\frac{x_i-\mu_B}{\sqrt{\sigma_B^2+\epsilon}}\end{aligned}
$$

- ì‹ ê²½ë§ ì¸µì„ ë§ì´ ìŒ“ì„ ë•Œ BNì„ í™œìš©í•˜ë©´ ì„±ëŠ¥ì´ ì˜¬ë¼ê°
- ë…¼ë¬¸ì˜ í•´ì„
    - í‰ê· ì„ 0ìœ¼ë¡œ, ë¶„ì‚°ì„ 1ë¡œ ë§Œë“¤ì–´ì¤Œìœ¼ë¡œì¨ internal covariate (feature) shiftê°€ ì¤„ì–´ë“ ë‹¤
    - ì´í›„ ë…¼ë¬¸ë“¤ì€ ì´ì— ëŒ€í•´ ë°˜ë°•

![2023-03-20-dl-basics-2-fig13](../../../assets/img/2023-03-20-dl-basics-2-fig13.png)

### ë§ˆë¬´ë¦¬

- ë„¤íŠ¸ì›Œí¬ë¥¼ ë§Œë“¤ê³ , ë°ì´í„°ì™€ ë¬¸ì œ ë“±ì´ ê³ ì •ë˜ì–´ ìˆì„ ë•Œ, ì´ëŸ¬í•œ ì •ê·œí™”(regularization) ê¸°ë²•ì„ ì‚¬ìš©í•´ë³´ë©´ì„œ ì¼ë°˜í™” ì„±ëŠ¥(generalization performance)ì„ ì‚´í•Œ

## ì°¸ê³ 
- [ë¶€ìŠ¤íŠ¸ì½”ìŠ¤ - ë”¥ëŸ¬ë‹ ê¸°ì´ˆ ë‹¤ì§€ê¸°](https://www.boostcourse.org/ai111){:target="_blank"}