---
layout: post
title: ìƒì„± ëª¨ë¸
category: cs
tags: ai
---

# [Deep Learning/ë”¥ ëŸ¬ë‹] ìƒì„± ëª¨ë¸

# Generative Models Part 1

## Introduction

ğŸ’¡ [Deep Generative Models](https://deepgenerativemodels.github.io/){:target="_blank"}

- What does it mean by learning a generative model?

### Learning a Generative Model

- Suppose that we are given images of dogs.
- We want to learn a probability distribution $$p(x)$$ such that
    - Generation: If we sample $$\tilde x\sim p(x)$$, $$\tilde x$$ should look like a dog.
    - Density estimation: $$p(x)$$ should be high if $$x$$ looks like a dog, and low otherwise.
        - This is also known as an explicit models.
- Then how can we represent $$p(x)$$?

### Basic Discrete Distributions

- Bernoulli distribution: (biased) coin flip
    - Sample space $$D=\{\text{Heads}, \text{Tails}\}$$.
    - Specify $$P(X=\text{Heads})=p$$. Then $$P(X=\text{Tails})=1-p$$.
    - Say $$X\sim\text{Ber}(p)$$.
- Categorical distribution: (biased) $$m$$-sided dice
    - Sample space $$D=\{1,\cdots,m\}$$.
    - Specify $$P(Y=i)=p_i$$ such that $$\sum_{i=1}^mp_i=1$$.
    - Say $$Y\sim\text{Cat}(p_1,\cdots,p_m)$$.

### Example

- Modeling a single pixel of an RGB image.
    - Color tuple $$(r,g,b)\sim p(R,G,B)$$
    - Number of cases = $$256\times256\times256$$
    - How many parameters do we need to specify?
        
        $$
        256\times256\times256-1
        $$
        

## Independence

### Example

- Suppose we have $$X_1,\cdots,X_n$$ of binary pixels (a binary image).
    - Number of cases = $$2^n$$
    - How many parameters do we need to specify?
        
        $$
        2^n-1
        $$
        
    
    â†’ Almost impossible to model a distribution
    

### Structure through Independence

- Suppose independencies among $$X_iâ€™s$$ for $$i=1,\cdots,m$$
    - Number of cases = $$2^n$$
    - However parameters we need to specify = $$n$$.
    
    â†’ Bad modeling that can represent a meaningful distribution
    

### Conditional Independence

- ìœ„ ë‘ assumptionsì˜ ì¤‘ê°„ ì–´ë”˜ê°€ì˜ íƒ€í˜‘ì ì„ ì°¾ê¸° ìœ„í•´..
- Three important rules
    - Chain rule
        
        $$
        p(x_1,\cdots,x_n)=p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)\cdots p(x_n|x_1,\cdots,x_{n-1})
        $$
        
    - Bayesâ€™ rule
        
        $$
        p(x|y)=\frac{p(x,y)}{p(y)}=\frac{p(y|x)p(x)}{p(y)}
        $$
        
    - Conditional independence
        
        If $$x\perp y|z$$, then
        
        $$
        p(x|y,z)=p(x,z)
        $$
        
- Using chain rule we obtain
    
    $$
    P(X_1,\cdots,X_n)=P(X_1)P(X_2|X_1)\cdots P(X_n|X_1,\cdots,X_{n-1})
    $$
    
- How many parameters?
    - For each term, $$P(X_i|X_{i-1},\cdots)$$: $$2^{i-1}$$ parameters needed
    - The total sum becomes $$1+2+\cdots+2^{n-1}=2^n-1$$.
- Now we suppose Markov assumption: $$X_{i+1}\perp X_1,\cdots,X_{i-1}\perp X_i$$, then the above equation becomes
    
    $$
    p(x_1,\cdots,x_n)=p(x_1)p(x_2|x_1)p(x_3|x_2)\cdots p(x_n|x_{n-1})
    $$
    
- How many parameters?
    
    $$
    2n-1
    $$
    
    - Hence by leveraging the Markov assumption, we get exponential reduction on the number of parameters.
    - Autoregressive models leverages this conditional independency.

## Autoregressive Models

- Suppose we have $$28\times28$$ binary pixels.
- Our goal is to learn $$P(X)=P(X_1,\cdots,X_{784})$$ over $$X\in\{0,1\}^{784}$$.
- Then how can we parametrize $$P(X)$$?
    - Letâ€™s use the chian rule to factor the joint distribution.
    - An autoregressive model:
        
        $$
        P(X_{1:784})=P(X_1)P(X_2|X_1)P(X_3|X_2)\cdots
        $$
        
    - Note that we need an ordering (e.g., raster scan order) of all random variables.

### NADE: Neural Autoregressive Density Estimator

![2023-03-22-dl-basics-5-fig1](../../../assets/img/dl-basics/2023-03-22-dl-basics-5-fig1.png)

- The probability distribution of $$i$$-th pixel is
    
    $$
    p(x_i|x_{1:i-1})=\sigma(\alpha_i\bold h_i+b_i)\;\;\text{where}\;\;\bold h_i=\sigma(W_{<i}x_{1:i-1}+\bold c)
    $$
    
- **NADE** is an explicit model that can compute the density of the given inputs.
- How can we compute the density of the given image?
    - Suppose that we have a binary image with 784 binary pixels i.e., $$\{x_1,x_2,\cdots,x_{784}\}$$.
    - Then the joint probability is computed by
        
        $$
        p(x_1,\cdots,x_{784})=p(x_1)\cdots p(x_{784}|x_{1:783})
        $$
        
        where each conditional probability $$p(x_i|x_{1:i-1})$$ is computed independently.
        
- In case of modeling continuous random variables, a mixture of Gaussian (MoG) can be used.

### Summary of Autoregressive Models

- Easy to sample from
    - Sample $$\tilde x_0\sim p(x_0)$$
    - Sample $$\tilde x_1\sim p(x_1|x_0=\tilde x_0)$$
    - and so forth (in a sequential manner, hence slow).
- Easy to compute probability $$p(x=\tilde x)$$
    - Compute $$p(x_0=\tilde x_0)$$
    - Compute $$p(x_1=\tilde x_1|x_0=\tilde x_0)$$
    - Multiply together (sum their logarithms)
    - and so on.
    - Ideally we can compute all these terms in parallel.
- Easy to be extended to continuous variables. E.g., we can choose mixture of Gaussians.

# Generative Models Part 2

## Maximum Likelihood Learning

![2023-03-22-dl-basics-5-fig2](../../../assets/img/dl-basics/2023-03-22-dl-basics-5-fig2.png)

- Given a training set of examples, we can cast the generative model learning process as finding the best-approximating density model from the model family.
- Then how can we evaluate the goodness of the approximation?
- KL-divergence ~ symmetric propertyë¥¼ ë§Œì¡±í•˜ì§€ ì•Šìœ¼ë‚˜ â€œê·¼ì‚¬ì ìœ¼ë¡œâ€ ë‘ ë¶„í¬ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ì˜ë¯¸í•˜ëŠ” ê²ƒìœ¼ë¡œ í•´ì„
    
    $$
    \begin{aligned}\mathcal D(P_\text{data}\Vert P_\theta)&=\mathbb E_{\bold x\sim P_\text{data}}\left[\log\left(\frac{P_\text{data}(\bold x)}{P_\theta(\bold x)}\right)\right]\\&=\int P_\text{data}(\bold x)\log\frac{P_\text{data}(\bold x)}{P_\theta(\bold x)}\;\text{d}\bold x\end{aligned}
    $$
    
- We can simplify the above as follows:
    
    $$
    \begin{aligned}\mathcal D(P_\text{data}\Vert P_\theta)&=\mathbb E_{\bold x\sim P_\text{data}}\left[\log\left(\frac{P_\text{data}(\bold x)}{P_\theta(\bold x)}\right)\right]\\&=\mathbb E_{\bold x\sim P_\text{data}}[\log P_\text{data}(\bold x)]-\mathbb E_{\bold x\sim P_\text{data}}[\log P_\theta(\bold x)]\end{aligned}
    $$
    
- As the first term does not depend on $$P_\theta$$, minimizing the KL-divergence is equivalent to maximizing the expected log-likelihood.
    
    $$
    \begin{aligned}\argmin_{P_\theta}\mathcal D(P_\text{data}\Vert P_\theta)&=\argmin_{P_\theta}-\mathbb E_{\bold x\sim P_\text{data}}[\log P_\theta(\bold x)]\\&=\argmax_{P_\theta}\mathbb E_{\bold x\sim P_\text{data}}[\log P_\theta(\bold x)]\end{aligned}
    $$
    
- Approxmiate the expected log-likelihood
    
    $$
    \mathbb E_{\bold x\sim P_\text{data}}\left[\log P_\theta(\bold x)\right]
    $$
    
    with the empirical log-likelihood
    
    $$
    \mathbb E_\mathcal D[\log P_\theta(\bold x)]=\frac{1}{|\mathcal D|}\int_{\bold x\in\mathcal D}\log P_\theta(\bold x)\;\text{d}\bold x
    $$
    
    - ë¶„í¬ $$P_\text{data}$$ì„ ì§ì ‘ì ìœ¼ë¡œ ì•Œ ìˆ˜ ì—†ëŠ” ìƒí™©ì—ì„œ ê´€ì¸¡í•œ ë°ì´í„°ì…‹ $$\mathcal D$$ê°€ ë¶„í¬ $$P_\text{data}$$ë¥¼ ê°€ì§„ë‹¤ê³  ê°€ì •í•˜ê³  parametrized distributionì„ í•™ìŠµí•˜ê² ë‹¤ëŠ” ê²ƒ
- Maximum likelihood learning is then
    
    $$
    \max_{P_\theta}\frac{1}{|\mathcal D|}\int_{\bold x\in\mathcal D}\log P_\theta(\bold x)\;\text{d}\bold x
    $$
    
- Variance of Monte Carlo estimate is high:
    
    $$
    V_P[\hat g]=V_P\left[\frac{1}{T}\sum_{t=1}^Tg(x^t)\right]=\frac{V_P[g(x)]}{T}
    $$
    
    - ë°ì´í„° ìˆ˜ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì„ ê²½ìš°, ì˜ˆì¸¡ì´ ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ

### Empirical Risk Minimization

- For maximum likelihood learning, empirical risk minimization (ERM) if often used.
- However, ERM often suffers from its overfitting.
    - Extreme case: The model remembers all training data
        
        $$
        p(x)=\frac{1}{|\mathcal D|}\sum_{i=1}^{|\mathcal D|}\delta(x,x_i)
        $$
        
- To achieve better generalization, we typically restrict the hypothesis space of distributions that we search over.
- However, it could deteriorate the performance of the generative model.

### Maximum Likelihood Learning

- Usually MLL is prone to underfitting as we often use simple parametric distributions such as spherical Gaussians.
- ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ í™•ë¥  ë¶„í¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸
- ìµœì í™”ë¥¼ ìœ„í•´ gradient descent ê¸°ë²•ì„ ì‚¬ìš©í•˜ë©° ë¯¸ë¶„ ê°€ëŠ¥í•œ log likelihoodë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ë° ì´ë•Œ ë¬´ë‚œí•˜ê²Œ ê°€ì •í•˜ëŠ” ê²ƒì´ Gaussian. ì‚¬ì‹¤ ìœ ì˜ë¯¸í•œ ë¶„í¬ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ê²ƒì´ 2014ë…„ Ian Goodfellowì´ ì œì•ˆí•œ GAN ì´ì „ê¹Œì§€ ë¶ˆê°€ëŠ¥í–ˆìŒ.
- What about other ways of measuring the similarity?
    - KL-divergence leads to maximum likelihood learning or Variational Autoencoder (VAE).
    - Jensen-Shannon divergence leads to Generative Adversarial Network (GAN).
    - Wasserstein distance leads to Wasserstein Autoencoder (WAE) or Adversarial Autoencoder (AAE).

## Latent Variable Models

<aside>
ğŸ’¡ D.Kingma, â€œVariational Inference and Deep Learning: A New Synthesis,â€ Ph.D Thesis

</aside>

### Is an autoencoder a generative model?

â†’ No. Just a model.

### Variational Autoencoder

- Objective
    
    $$
    \max p_\theta(x)
    $$
    
- Variational Inference (VI)
    - The goal of VI is to optimize the variational distribution the best matches the posterior distribution.
        - Posterior distribution: $$p_\theta(z|x)$$
        - Variational distribution: $$q_\phi(z|x)$$
    - In particular, we want to find the variational distribution that minimizes the KL-divergence between the true posterior.

![2023-03-22-dl-basics-5-fig3](../../../assets/img/dl-basics/2023-03-22-dl-basics-5-fig3.png)

- MLE
    
    $$
    \begin{aligned}
    \overbrace{\log p_\theta(x)}^\text{maximum likelihood learning}
    &=\int_z\underbrace{q_\phi(z|x)}_{\text{For\;any}\;q_\phi}\log p_\theta(x)\mathrm{d}x
    \\&=\mathbb E_{z\sim q_\phi(z|x)}\left[\log\frac{p_\theta(x)p_\theta(z|x)}{p_\theta(z|x)}\right]
    \\&=\mathbb E_{z\sim q_\phi(z|x)}\left[\log\frac{p_\theta(x,z)}{p_\theta(z|x)}\right]
    \\&=\mathbb E_{z\sim q_\phi(z|x)}\left[\log\frac{p_\theta(x,z)q_\phi(z|x)}{p_\theta(z|x)q_\phi(z|x)}\right]
    \\&=\mathbb E_{z\sim q_\phi(z|x)}\left[\log\frac{p_\theta(x,z)}{q_\phi(z|x)}\right]+\mathbb E_{z\sim q_\phi(z|x)}\left[\log\frac{q_\phi(z|x)}{p_\theta(z|x)}\right]
    \\&=\underbrace{\mathbb E_{z\sim q_\phi(z|x)}\left[\log\frac{p_\theta(x,z)}{q_\phi(z|x)}\right]}_{\text{ELBO}\;\uparrow}+\underbrace{D_{KL}(q_\phi(z|x)\Vert p_\theta(z|x))}_{\text{Variational Gap}\;\downarrow}
    \end{aligned}
    $$
    
    - ELBO: Evidence Lower Bound
    - True posterior $$q_\phi(z|x)$$ëŠ” ì¡´ì¬ì„±ì€ ì•Œê³  ìˆìœ¼ë‚˜ ìˆ˜ì‹ìœ¼ë¡œ í’€ì–´ë‚´ì–´ ê³„ì‚°í•˜ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•œ í•­
    - ëª©ì ì€ $$p_\theta$$ë¥¼ $$q_\phi$$ì— ê·¼ì‚¬í•˜ê³ ì í•˜ëŠ” ê²ƒ
    - ì´ë•Œ ELBOëŠ” tractable quantityì´ë¯€ë¡œ ì´ë¥¼ ìµœëŒ€í™”í•˜ì—¬ variational gapì„ ìƒëŒ€ì ìœ¼ë¡œ ì¤„ì´ê³ ì í•˜ëŠ” ê²ƒ

### Evidence Lower Bound

$$
\begin{aligned}
\underbrace{\mathbb E_{z\sim q_\phi(z|x)}\left[\log\frac{p_\theta(x,z)}{q_\phi(z|x)}\right]}_{\text{ELBO}\;\uparrow}
&=\int\log\frac{p_\theta(x|z)p(z)}{q_\phi(z|x)}q_\phi(z|x)\mathrm dz
\\&=\underbrace{\mathbb E_{q_\phi(z|x)}[p_\theta(x|z)]}_\text{reconstruction\;term}-\underbrace{D_{KL}(q_\phi(z|x)\Vert p(z))}_\text{prior\;fitting\;term}
\end{aligned}
$$

- Reconstruction term: encoder $$q_\phi$$ì™€ decoder $$p_\theta$$ë¥¼ ëª¨ë‘ í†µê³¼í•œ ê²ƒìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆìŒ
- Prior fitting term â€” enforces the latent distribution to be similar to the prior distribution $$p(z)$$.

### Key Limitations

- Intractable model (hard to evaluate likelihood)
- The prior fitting term should be differentiable ($$\because$$ gradient descent method), hence it is hard to use diverse latent prior distributions.
- In most cases, we use an isotropic Gaussian where we have a closed-form for the prior fitting term.
    
    $$
    D_{KL}(q_\phi(z|x)\Vert \mathcal N(0,I))=\frac{1}{2}\sum_{i=1}^{|\mathcal D|}(\sigma_{z_i}^2+\mu_{z_i}^2-\log(\sigma_{z_i}^2)-1)
    $$
    
- Prior fitting termì„ gradient descentë¡œ ìµœì í™”í•˜ê¸° ìœ„í•´ì„œëŠ” isotropic Gaussianì„ ê°€ì •í•  ìˆ˜ë°–ì— ì—†ê¸° ë•Œë¬¸ì— Gaussianì„ ì‚¬ìš©
- ìƒì„±ëœ ì´ë¯¸ì§€ì˜ í€„ë¦¬í‹°ë¡œ ë³¼ ë•Œ, ì¶”ì²œë˜ì§€ ì•ŠëŠ” ë°©ë²•

## Generative Adversarial Networks

![2023-03-22-dl-basics-5-fig4](../../../assets/img/dl-basics/2023-03-22-dl-basics-5-fig4.png)

$$
\min_G\max_DV(D,G)=\mathbb E_{\bold x\sim p_\text{data}(\bold x)}[\log D(\bold x)]+\mathbb E_{\bold z\sim p_{\bold z}(\bold z)}[\log(1-D(G(\bold z)))]
$$

### GAN objective

- GAN is a two player minimax game between generator and discriminator.
    - Discriminator objective:
        
        $$
        \max_DV(G,D)=\mathbb E_{\bold x\sim p_\text{data}(\bold x)}[\log D(\bold x)]+\mathbb E_{\bold x\sim p_G(\bold z)}[\log(1-D(\bold x))]
        $$
        
    - The optimal discriminator is
        
        $$
        D_G^*(\bold x)=\frac{p_\text{data}(\bold x)}{p_\text{data}(\bold x)+p_G(\bold x)}
        $$
        
    - Generator objective:
        
        $$
        \min_GV(G,D)=\mathbb E_{\bold x\sim p_\text{data}}[\log D(\bold x)]+\mathbb E_{\bold x\sim p_G}[\log(1-D(\bold x))]
        $$
        
    - Plugging the optimal discriminator into the above generator objective equation we obtain
        
        $$
        \begin{aligned}
        V(G,D_G^*(\bold x))
        &=\mathbb E_{\bold x\sim p_\text{data}}\left[\log \frac{p_\text{data}(\bold x)}{p_\text{data}(\bold x)+p_G(\bold x)}\right]+\mathbb E_{\bold x\sim p_G}\left[\log\frac{p_G(\bold x)}{p_\text{data}(\bold x)+p_G(\bold x)}\right]
        \\&=\mathbb E_{\bold x\sim p_\text{data}}\left[\log \frac{p_\text{data}(\bold x)}{\frac{p_\text{data}(\bold x)+p_G(\bold x)}{2}}\right]+\mathbb E_{\bold x\sim p_G}\left[\log\frac{p_G(\bold x)}{\frac{p_\text{data}(\bold x)+p_G(\bold x)}{2}}\right]-\log4
        \\&=\underbrace{D_{KL}\left[p_\text{data},{\frac{p_\text{data}+p_G}{2}}\right]+D_{KL}\left[p_G,{\frac{p_\text{data}+p_G}{2}}\right]}_{2\times\text{Jenson-Shannon\;Divergence\;(JSD)}}-\log4
        \\&=2D_{JSD}[p_\text{data},p_G]-\log4
        \end{aligned}
        $$
        

### Deep Convolutional GAN

![2023-03-22-dl-basics-5-fig5](../../../assets/img/dl-basics/2023-03-22-dl-basics-5-fig5.png)

## Diffusion Models

<aside>
ğŸ’¡ Ho et al., â€œDenoising Diffusion Probabilistic Models,â€ 2020

</aside>

- Diffusion models progressively generate images from noise.
- ë…¼ë¬¸ ì´í•´ë¥¼ ìœ„í•œ lecture note ë§í¬: [https://github.com/sjchoi86/2022-1-deep-learning-applications/blob/main/lecture note/08. Generative model (DDPM).pdf](https://github.com/sjchoi86/2022-1-deep-learning-applications/blob/main/lecturenote/08.Generativemodel(DDPM).pdf)
- DDPM paperëŠ” 1000ë²ˆì˜ stepì„ ë°ŸìŒ. í•œ ì´ë¯¸ì§€ ìƒì„±ì— ëª‡ ë¶„ì”© ì†Œìš”.
- ì¥ì ? ì„±ëŠ¥ì´ ì¢‹ìŒ. ë¹„íš¨ìœ¨ì ìœ¼ë¡œ ë³´ì´ê³  êµ¬í˜„í•˜ê¸°ëŠ” ë”ë”ìš± ì–´ë ¤ìš°ë‚˜, ì›”ë“±íˆ ì¢‹ì€ ì„±ëŠ¥.
- Autoregressive model ì„±ëŠ¥ë„ ì¢‹ë‹¤ëŠ” ê²ƒì´ ì•Œë ¤ì§€ë©´ì„œ 2022ë…„ ê¸°ì¤€ ì—ì¹˜ë½ë’¤ì¹˜ë½í•˜ëŠ” ìƒí™©.

### Diffusion Models

![2023-03-22-dl-basics-5-fig6](../../../assets/img/dl-basics/2023-03-22-dl-basics-5-fig6.png)

- Forward (diffusion) process progressively injects noise to an image.
    
    $$
    p_\theta(\bold x_{t-1}|\bold x_t):=\mathcal N(\bold x_{t-1};\mu_\theta(\bold x_t,t),\Sigma_\theta(\bold x_t,t))
    $$
    
- The reverse process is learned in such a way to denoise the perturbed image back to a clean image.
- DDPM ê°„ë‹¨ ìš”ì•½: êµ‰ì¥íˆ ì˜¤ëœ ìŠ¤í…ì„ ê±°ì³ noise vectorë¥¼ original imageë¡œ ê°œì„ í•´ë‚˜ê°€ëŠ” ê³¼ì •

### DALL-E 2

![2023-03-22-dl-basics-5-fig7](../../../assets/img/dl-basics/2023-03-22-dl-basics-5-fig7.png)

- ì´ë¯¸ì§€ì˜ íŠ¹ì • ë¶€ë¶„ì„ editing í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ ì¸ìƒì 

## ì°¸ê³ 
- [ë¶€ìŠ¤íŠ¸ì½”ìŠ¤ - ë”¥ëŸ¬ë‹ ê¸°ì´ˆ ë‹¤ì§€ê¸°](https://www.boostcourse.org/ai111){:target="_blank"}
