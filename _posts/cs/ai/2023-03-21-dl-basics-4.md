---
layout: post
title: ì‹œí€€ìŠ¤ ëª¨ë¸
category: cs
tags: ai
---

# [Deep Learning/ë”¥ ëŸ¬ë‹] ì‹œí€€ìŠ¤ ëª¨ë¸

## Recurrent Neural Networks (RNNs)

### Sequence Models

- Naive sequence model
- ì‹œí€€ìŠ¤ ë°ì´í„°ëŠ” ê¸¸ì´ê°€ ì–´ë–»ê²Œ ë ì§€ ëª¨ë¥¸ë‹¤ëŠ” íŠ¹ì§•ì´ ìˆìŒ â†’ ì¼ë°˜ CNN ëª¨ë¸ì„ ë°”ë¡œ ì‚¬ìš©í•˜ê¸° í˜ë“¦

#### Autoregressive model

![2023-03-21-dl-basics-4-fig1](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig1.png)

- ê³¼ê±°ì˜ time spanì„ ê³ ì •

#### Markov model (first-order autoregressive model)

![2023-03-21-dl-basics-4-fig2](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig2.png)

- í˜„ì¬ëŠ” ë°”ë¡œ ì „ ê³¼ê±°ì—ë§Œ dependentí•˜ë‹¤ëŠ” ê°€ì •
    
    $$
    p(x_i|x_{i-1},\cdots,x_1)\overset{\tiny\text{assume}}{=}p(x_i|x_{i-1})
    $$
    
    - ì •ë³´ëŸ‰ ì†ì‹¤ì´ í¼
- Generative modelì—ì„œ ë§ì´ í™œìš©ë˜ëŠ” autoregressive model !

#### Latent autoregressive model

![2023-03-21-dl-basics-4-fig3](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig3.png)

- ê³¼ê±° ì •ë³´ì˜ summaryê°€ ë‹´ê¸´ hidden (latent) state $$h$$

### Recurrent Neural Network (RNN)

![2023-03-21-dl-basics-4-fig4](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig4.png)

- ë¬¸ì œì 
    - Short-term dependencies: fixed ruleë¡œ ê³¼ê±°ì˜ ì •ë³´ë¥¼ ì·¨í•©í•˜ë‹¤ë³´ë‹ˆ ë¯¸ë˜ì˜ sequenceì— ê³¼ê±° ì •ë³´ê°€ ì˜ ì „ë‹¬ë˜ì§€ ì•ŠëŠ” í˜„ìƒ
        
        â†’ í•´ê²°ì„ ìœ„í•´ Long Short Term Memory (LSTM) ë“±ì¥
        
- í•™ìŠµì´ ì–´ë ¤ìš´ ì´ìœ ?
    
    ![2023-03-21-dl-basics-4-fig5](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig5.png)
    
    - ë„¤íŠ¸ì›Œí¬ë¥¼ í’€ê²Œ ë˜ë©´ widthê°€ ë§¤ìš° ì»¤ì§€ê²Œ ë¨

### Long Short Term Memory (LSTM)

- Vanilla RNN
    
    ![2023-03-21-dl-basics-4-fig6](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig6.png)
    
- LSTM
    
    ![2023-03-21-dl-basics-4-fig7](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig7.png)
    
    - ì™œ long-term dependencyë¥¼ ì¡ëŠ” ë° ë„ì›€ì´ ë˜ëŠ”ì§€ ì´í•´í•˜ëŠ” ê²ƒ í•„ìš”

![2023-03-21-dl-basics-4-fig8](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig8.png)

1. Forget gate
    
    ![2023-03-21-dl-basics-4-fig9](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig9.png)
    
    - Decide which information to throw away
    
    $$
    f_t=\sigma(W_f\cdot[h_{t-1},x_t]+b_f)
    $$
    
2. Input gate
    
    ![2023-03-21-dl-basics-4-fig10](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig10.png)
    
    - Decide which information to store in the cell state
    
    $$
    \begin{aligned}
    i_t&=\sigma(W_i\cdot[h_{t-1},x_t]+b_i)\\\tilde C_t&=\tanh(W_C\cdot[h_{t-1},x_t]+b_C)
    \end{aligned}
    $$
    
3. Update gate
    
    ![2023-03-21-dl-basics-4-fig11](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig11.png)
    
    - Update the cell state
    
    $$
    C_t=f_t*C_{t-1}+i_t*\tilde C_t
    
    $$
    
4. Output gate
    
    ![2023-03-21-dl-basics-4-fig12](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig12.png)
    
    - Make output using the updated cell state
    
    $$
    \begin{aligned}
    o_t&=\sigma(W_o\cdot[h_{t-1},x_t]+b_o)\\h_t&=o_t*\tanh(C_t)
    \end{aligned}
    $$
    

### Gated Recurrent Unit (GRU)

![2023-03-21-dl-basics-4-fig13](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig13.png)

- Simpler architecture with two gates (reset gate & update gate)
- No cell state, just hidden state
- ë„¤íŠ¸ì›Œí¬ íŒŒë¼ë¯¸í„° ìˆ˜ê°€ LSTMì— ë¹„í•´ ì ì€ë°, ì ì€ íŒŒë¼ë¯¸í„°ë¡œë„ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë‚¸ë‹¤ë©´, generalization performanceê°€ ì˜¬ë¼ê°„ë‹¤ëŠ” ê´€ì ì—ì„œ GRUë„ ë§ì´ ì‚¬ìš©í–ˆìŒ
- Transformer ì´í›„ RNN ê¸°ë°˜ ì•„í‚¤í…ì²˜ëŠ” ë§ì´ ì“°ì§€ëŠ” ì•Šê²Œ ë¨

## Transformer

<aside>
ğŸ’¡ Why is sequence modeling hard?

</aside>

![2023-03-21-dl-basics-4-fig14](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig14.png)

- Attention is All You Need, NeurIPS 2017.
- Transformer is the first sequence transduction model based entirely on attention.

![2023-03-21-dl-basics-4-fig15](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig15.png)

- From a birdâ€™s eye-view, this is what the Transformer does for machine translation tasks.
- NMT ë¿ë§Œ ì•„ë‹ˆë¼ ë‹¤ë¥¸ sequence model tasksì—ë„ ì ìš© ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜. ì´ë¯¸ì§€ ë¶„ë¥˜, íƒì§€ ë“±ì—ë„ í™œìš©ë¨

### Transformer

![2023-03-21-dl-basics-4-fig16](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig16.png)

#### êµ¬ì¡° ì„¤ëª…

![2023-03-21-dl-basics-4-fig17](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig17.png)

- If we glide down a little bit, this is what the Transformer does.

![2023-03-21-dl-basics-4-fig18](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig18.png)

- ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ë‹¨ì–´ ìˆ˜ì™€ ì¶œë ¥ ì‹œí€€ìŠ¤ì˜ ë‹¨ì–´ ìˆ˜ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
- ì…ì¶œë ¥ ì‹œí€€ìŠ¤ ê°ê°ì˜ ë„ë©”ì¸ì€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
- ì¬ê·€ì ì¸ ì•„í‚¤í…ì²˜ê°€ ì•„ë‹ˆë¼ í•œ ë²ˆì— ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°. ë‹¤ë§Œ, ìƒì„± ëª¨ë¸ì˜ ê²½ìš° autoregressive íŠ¹ì„±ì„ ê°€ì§€ë©° ëŒì•„ê°
- ëª©í‘œ
    - ë‹¨ì–´ $$n$$ê°œê°€ ì–´ë–»ê²Œ encoderì—ì„œ ì²˜ë¦¬ë˜ëŠ”ì§€ ì´í•´
    - Encoder-decoder ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ í†µí•´ ì–´ë–¤ ì •ë³´ê°€ ì˜¤ê°€ëŠ”ì§€ ì´í•´
    - Decoderê°€ ì–´ë–»ê²Œ generationì„ í•  ìˆ˜ ìˆëŠ”ì§€ ì´í•´

![2023-03-21-dl-basics-4-fig19](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig19.png)

- The Self-Attention in both encoder and decoder architectures is the cornerstone of Transformer.

![2023-03-21-dl-basics-4-fig20](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig20.png)

- First, we represent each word with some embedding vectors.

![2023-03-21-dl-basics-4-fig21](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig21.png)

- Then Transformer encodes each word to feature vectors with Self-Attention.
- ë‹¨ì–´ $$n$$ê°œê°€ ì£¼ì–´ì§€ë©´ ê°ê°ì˜ $$n$$ê°œì˜ ë‹¨ì–´ì˜ ì„ë² ë”© ë²¡í„°ë“¤ì„ ì°¾ì•„ì£¼ëŠ” ì—­í• 
- ì´ë•Œ ë‹¨ì¼ ë‹¨ì–´ ì •ë³´ë¿ë§Œ ì•„ë‹ˆë¼ ë¬¸ë§¥ ë‹¨ì–´ ì •ë³´ê¹Œì§€ ê°™ì´ ê³ ë ¤í•¨
    
    â†’ Dependency ì¡´ì¬
    

![2023-03-21-dl-basics-4-fig22](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig22.png)

- Suppose we encode two words: Thinking and Machines.
- Self-Attention at a high level
    - The animal didnâ€™t cross the street because it was too tired.

![2023-03-21-dl-basics-4-fig23](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig23.png)

- 3ê°€ì§€ ë²¡í„°(3ê°œì˜ ì‹ ê²½ë§)ë¥¼ ë§Œë“¤ì–´ ëƒ„

![2023-03-21-dl-basics-4-fig24](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig24.png)

![2023-03-21-dl-basics-4-fig25](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig25.png)

- Score vector: encoding í•˜ê³ ìí•˜ëŠ” ë‹¨ì–´ì˜ query ë²¡í„°ì™€ ë‚˜ë¨¸ì§€ $$n$$ê°œ ë‹¨ì–´ì˜ key ë²¡í„°ë¥¼ ë‚´ì í•˜ì—¬ ì–¼ë§ˆë‚˜ align ë˜ì–´ìˆëŠ”ì§€, ê´€ê³„ê°€ ì–´ë–¤ì§€ ì‚´í•Œ
- ì˜ˆë¥¼ ë“¤ì–´, Thinkingì„ encodingí•  ë•Œ ë‚˜ë¨¸ì§€ ë‹¨ì–´ë“¤ ì¤‘ ì–´ë–¤ ë‹¨ì–´ì™€ interactionì´ ë§ì´ ì¼ì–´ë‚˜ì•¼ í•˜ëŠ”ì§€ ì •ë³´ë¥¼ attention scoreë¥¼ ê³„ì‚°í•˜ì—¬ ë‹´ìŒ

![2023-03-21-dl-basics-4-fig26](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig26.png)

- Then we compute the attention weights by sacling followed by softmax.

![2023-03-21-dl-basics-4-fig27](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig27.png)

1. ì„ë² ë”© ë²¡í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ
2. ê·¸ ì„ë² ë”© ë²¡í„°ì— ëŒ€í•´ ê°ê°ì˜ ì‹ ê²½ë§ì„ ê±°ì³ $$q,k,v$$ (query, key, value) ë²¡í„°ë¥¼  ë§Œë“  ë’¤
3. query ë²¡í„°ì™€ key ë²¡í„°ì˜ ë‚´ì ìœ¼ë¡œ attention ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³ 
4. activation functionì˜ íŠ¹ì„±ì„ ê³ ë ¤í•´ scalingì„ ì§„í–‰í•œ ê°’ì„ softmaxë¥¼ ì·¨í•˜ì—¬
5. ê° ì„ë² ë”©ë“¤ì˜ value ë²¡í„°ë“¤ì˜ weightë¡œ ì‚¼ì•„ weighted sumì„ ì‚¬ìš©
- query, key ë²¡í„°ëŠ” ì°¨ì›ì´ ê°™ì•„ì•¼ í•˜ë‚˜, value ë²¡í„°ëŠ” ì°¨ì›ì´ ë‹¬ë¼ë„ ë¨

![2023-03-21-dl-basics-4-fig28](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig38.png)

- Calculate Q, K, and V from X in a matrix form.

![2023-03-21-dl-basics-4-fig29](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig39.png)

#### ì™œ ì˜ ë˜ëŠ”ê°€?

- ì´ë¯¸ì§€ ë°ì´í„°ê°€ MLPë‚˜ CNN ì•„í‚¤í…ì²˜ë¥¼ í†µê³¼í•˜ëŠ” ê²½ìš° fixed inputì— ëŒ€í•´ outputë„ ê³ ì •ë¨
- Transformerì˜ ê²½ìš° ë¬¸ë§¥ ì‹œí€€ìŠ¤ì— ë”°ë¼ ë‹¨ì–´ì˜ ì„ë² ë”© ë²¡í„°ê°€ ë‹¬ë¼ì§€ê²Œ ë¨ â†’ more flexible architecture; ample expressions of each element
- ë•Œë¬¸ì— ë” ë§ì€ ì—°ì‚°ëŸ‰ì´ ìš”êµ¬ëœë‹¤ê³  í•´ì„í•  ìˆ˜ë„ ìˆìŒ
- ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ $$n$$ì´ ì£¼ì–´ì§€ë©´ ê¸°ë³¸ì ìœ¼ë¡œ $$n\times n$$ì˜ attention mapì„ ë§Œë“¤ì–´ì•¼ í•¨ â†’ time complexityê°€ $$O(n^2)$$ì´ë¯€ë¡œ $$n$$ì„ í‚¤ìš°ëŠ” ë° í•œê³„ê°€ ìˆëŠ” ê²ƒì´ Transformerì˜ í•œê³„ ($$n$$ê°œì˜ ìš”ì†Œë“¤ì„ í•œ ë²ˆì— ì²˜ë¦¬í•´ì•¼ í•˜ë¯€ë¡œ)
- RNNì˜ ê²½ìš°, ì˜ˆì»¨ëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ $$n=1000$$ì´ë©´ 1000ë²ˆ RNNì„ ëŒë¦¬ë©´ ë¨.

#### Multi-headed attention

![2023-03-21-dl-basics-4-fig30](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig30.png)

- Multi-headed attention (MHA) allows Transformer to focus on different positions.

![2023-03-21-dl-basics-4-fig31](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig31.png)

- If eight heads are used, we end up getting eight different sets of encoded vectors (attention heads).

![2023-03-21-dl-basics-4-fig32](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig32.png)

- We simply pass thme through additional (learnable) linear map.

![2023-03-21-dl-basics-4-fig33](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig33.png)

#### Positional encoding

![2023-03-21-dl-basics-4-fig34](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig34.png)

- ìˆœì„œ ì •ë³´ë¥¼ ë„£ì–´ì£¼ê¸° ìœ„í•œ ëª©ì 
- Why? self-attentionì€ order-independent

![2023-03-21-dl-basics-4-fig35](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig35.png)

- 4-dimensional encoding case

![2023-03-21-dl-basics-4-fig36](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig36.png)

- 512-dimensional encoding

![2023-03-21-dl-basics-4-fig37](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig37.png)

- Update on positional encoding (July 2020)

![2023-03-21-dl-basics-4-fig38](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig38.png)

![2023-03-21-dl-basics-4-fig39](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig39.png)

#### Decoder side

![2023-03-21-dl-basics-4-fig40](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig40.png)

- Transformer transfers key (K) and value (V) of the topmost encoder to the decoder.

![2023-03-21-dl-basics-4-fig41](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig41.png)

- The output sequence is generated in an autoregressive manner.
- In the decoder, the self-attention layer is only allowed to attend to earlier attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values from the encoder stack.

![2023-03-21-dl-basics-4-fig42](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig42.png)

- The final layer converts the stack of decoder outputs to the distribution over words.

#### Vision Transformer

![2023-03-21-dl-basics-4-fig43](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig43.png)

#### DALL-E

![2023-03-21-dl-basics-4-fig44](../../../assets/img/dl-basics/2023-03-21-dl-basics-4-fig44.png)

- An armchair in the shape of an avocado


## ì°¸ê³ 
- [ë¶€ìŠ¤íŠ¸ì½”ìŠ¤ - ë”¥ëŸ¬ë‹ ê¸°ì´ˆ ë‹¤ì§€ê¸°](https://www.boostcourse.org/ai111){:target="_blank"}
- [The Illustrated Transformer by Jay Alammar](https://jalammar.github.io/illustrated-transformer/){:target="_blank"}